{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNOStgMQu1MLEdIgNmg8+q6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Hugging Face: Basic"],"metadata":{"id":"_ok_HY-5uw5z"}},{"cell_type":"markdown","source":["Hugging Face is a company and open-source community that provides tools to buid, deploy and share ML models.\n","\n","Mission:\n","- To democratize good machine learning, one commit at a time.\n","- Open-source AI by providing a one-stop-shop of resources, ranging from models (+30k), datasets (+5k), ML demos (+2k) and libraries.\n","\n","Key services:\n","- **Models**: A collection of thousands of pre-trained models for different tasks.\n","- **Datasets**: A library of datasets covering various domains and languages.\n","- **Spaces**: A platform for hosting and sharing interactive web applications and demo.\n","- **Inference API**: A fully managed service that allows you to deploy any Hugging Face model on scalable and secure infrastructure.\n","- **AutoTrain**: A no-code solution that automatically searches and trains the best model for your data and task."],"metadata":{"id":"GmZP4d3mu1ZO"}},{"cell_type":"markdown","source":["## Topics:\n","- Transformer overview\n","- transformer library\n","- transformer.pipeline"],"metadata":{"id":"-WUk0CIB11Pu"}},{"cell_type":"markdown","source":["Natural Language Processing\n","\n","- NLP is a field that combines linguistics and machine learning to understand human language. It involves tasks like classifying sentences based on sentiment or grammar, generating text, extracting answers from texts, and handling challenges in speech recognition and computer vision.\n","\n"],"metadata":{"id":"33Z1lDFN2Kfc"}},{"cell_type":"markdown","source":["## Transformer history\n","\n","- June 2017: The Transformer architecture was introduced by google \"Attention is all you need.\" (https://arxiv.org/abs/1706.03762).\n","- June 2018: GPT, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results\n","- October 2018: BERT, another large pretrained model, this one designed to produce better summaries of sentences.\n","- February 2019: GPT-2, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns.\n","- October 2019: DistilBERT, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT’s performance.\n","- October 2019: BART and T5, two large pretrained models using the same architecture as the original Transformer model.\n","- May 2020, GPT-3, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called zero-shot learning).\n","\n","Broadly, they can be grouped into three categories:\n","\n","- GPT-like (also called auto-regressive Transformer models)\n","- BERT-like (also called auto-encoding Transformer models)\n","- BART/T5-like (also called sequence-to-sequence Transformer models)"],"metadata":{"id":"VfK5bsPov-cd"}},{"cell_type":"markdown","source":["![Transformers Chrono](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg)"],"metadata":{"id":"jgUiy31YwpjT"}},{"cell_type":"markdown","source":["![Transformers Chrono](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png)"],"metadata":{"id":"oli7lQABx64O"}},{"cell_type":"markdown","source":["**Large Language Model (LLM)** is the model that are trained on massive amounts of text data to understand contextual relationshipss between words.\n","\n","**Pretrained Models**: Pretrained models are LLMs that have been trained on large corpora of text data. These models are trained on diverse sources which capable of generating coherent and contextually appropriate text.\n","\n","**Fine-tuning** is the process of taking a pretrained model and further training it on a specific task or domain. Fine-tuned models are useful in various applications such as language translation, chatbots, question-answering systems, and more.\n"],"metadata":{"id":"z1mX8qgjpFEC"}},{"cell_type":"markdown","source":["Model components:\n","\n","**Encoder**: The encoder receives an input and builds its features.\n","\n","**Decoder**: The decoder uses the features along with other inputs to generate a target sequence.\n","\n","**Attention layers**: It is designed to mimic the human cognitive mechanism of selectively focusing. Attention layers enable models to assign different weights or importance to different parts of the input sequence.\n","\n","**Model head**: the additional layers added on top of the base transformer architecture to tailor the model for a specific task, allowing it to transform the learned representations into task-specific predictions or outputs."],"metadata":{"id":"HxAF8FBDrkfj"}},{"cell_type":"markdown","source":["The original Transformer architecture\n","![Transformers Chrono](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg)"],"metadata":{"id":"LD3pLiTPuZq8"}},{"cell_type":"markdown","source":["## Model archetecture:"],"metadata":{"id":"Xgo66SP_aXqm"}},{"cell_type":"markdown","source":["### **Encoder-only models** (auto-encoding models)\n","\n","- Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence.\n","- The pretraining of these models is usually trained to predict randsom masking.\n","- Models: ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa\n","- Good for tasks that require understanding of the input, such as sentence classification and named entity recognition."],"metadata":{"id":"5XEvrFvvabze"}},{"cell_type":"markdown","source":["### **Decoder-only models** (auto-regressive models)\n","\n","- Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence.\n","- The pretraining of decoder models is trained to predicting the next word in the sentence.\n","- Models: CTRL, GPT, GPT-2, Transformer XL\n","- Good for generative tasks such as text generation."],"metadata":{"id":"sric1FNRbThd"}},{"cell_type":"markdown","source":["### **Encoder-decoder models** (sequence-to-sequence models)\n","\n","- At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.\n","- Pretrained by replacing random spans of text with a single mask special word and the objective is then to predict the text that this mask word replaces.\n","- Models: BART, mBART, Marian, T5\n","- Good for generative tasks that require an input, such as summarization, translation, or generative question answering."],"metadata":{"id":"HnHudzbVsbg0"}},{"cell_type":"markdown","source":["## Bias and limitations\n","\n","- Pretrained is trained on data on the internet.\n","- Easily generate sexist, racist, or homophobic content.\n","- Fine-tuning the model on your data won’t make this intrinsic bias disappear."],"metadata":{"id":"vjM6lcBJeIga"}},{"cell_type":"markdown","source":["## Reference:\n","\n","- Hugging Face NLP Course\n","  - https://huggingface.co/learn/nlp-course/chapter1/1\n","\n","- datacamp: An Introduction to Using Transformers and Hugging Face\n","  - https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face"],"metadata":{"id":"8exTCvK1U_xq"}}]}